---
title: "Output Review"
teaching: 0
exercises: 0
questions:
- "Why is output review an essential component of curating code for reproducibility?"
- "What are outputs and why are they important?"
- "What are the essential steps in output and manuscript review?"
objectives:
- "To know the importance of reviewing output files and including them in the reproducibility package"
- "Identify and verify outputs"
- "Review manuscripts"
keypoints:
- "Review the output and include a copy in the reproducibility package."
- "Ensure the output files are independently understandable and re-user friendly."
- "Account for every table, figure, and in-text numbers on the manuscript."
- "Identify the program (code) and specify the line number in the code that created them."
- "Ensure the code reproduces the tables, figures, and in-text numbers on the manuscript."
---
You reviewed files, documentation, and data. You ran the code and confirmed that it is error-free. You are now ready for a key part of curating for reproducibility: comparing the output produced by the code with the findings reported in the article!

## Manuscript Results

An initial step in assessing reproducibility is inspecting the manuscript to identify all analysis results appearing in tables, graphs, figures, and in-text references.  For the purposes of this lesson, we use the term “manuscript” to refer to a working paper, draft, preprint, article under review, or a published article.  

Inspecting the manuscript requires a close reading of the entire document, which may include appendices and supplementary materials.  The goal of this task is to confirm that the code includes the commands needed to reproduce tables and figures, as well as results appearing as in-line text and not referenced in tables, figures, and/or graphs.

To facilitate this process, curators may highlight sections of the manuscript where results appear. Doing so will make it easier to compare code outputs to the results in the manuscript.  

> ## Exercise: Identifying Manuscript Results
>
> Perform a manuscript inspection by doing a close reading of the entire manuscript and highlighting sections of the manuscript that present analysis results.  Be sure that you highlight all figures, tables, graphs, and in-text numbers.
>
> > ## Solution
> >
> > solution
> >
> {: .solution}
{: .challenge}

## Code Outputs

When code executes successfully, the program will display the results of the computations, i.e., code outputs.  Sometimes, these outputs can be difficult to interpret by non-domain experts, or simply because the outputs lack the neat formatting of tables, figures, and graphs seen in the manuscript. In other cases, code may generate outputs in an order that does not align with the orientation of results in the manuscript.  Output files and log files can make it easier to locate and interpret code outputs for code output and manuscript review purposes.

### Output files  

Depending on how it is written, the code may include commands or scripts that produce output files that contain an image of a graph, figure, table, or some other result, which is often embedded in the manuscript.  These output files are standalone artifacts that are included in the research compendium. Below are some output file examples:

[IMAGE]  

### The log file  

The log file is of particular importance in the reproducibility assessment process because it not only can present code outputs in a readable format, but also it serves as a record of the analytical workflow.  The log file provides a written log of the computational events that occurred during a given session during which the program was executed.  The log file can be generated automatically or manually, and typically serves as a reference for researchers as they write or revise the manuscript.  

For curators, the log file can be useful because results are written into the log (with the exception of graphs), which can be used to reference code outputs when assessing whether or not the research compendium can be used to reproduce manuscript results.  Below is an example log:

[IMAGE of annotated log file]

> ## Spotlight: When Data are Restricted
>
> text
{: .callout}

> ## Identifying Code Outputs
>
> In the sample research compendium, the researcher has included a log file that includes code outputs.  Review the log closely to identify code outputs. Highlight all sections of the log that show code outputs.
>
> > ## Solution
> >
> > solution
> >
> {: .solution}
{: .challenge}

## Comparing Code Outputs to Reported Results

The examples below show the output of specific code blocks alongside their corresponding results presented in the manuscript.  

[Annotated image]  

[Annotated image]  

[Annotated image]  

### Verifying Reproducibility  

Verifying reproducibility requires that the results reported in the manuscript are compared against outputs generated by the code.  Any inconsistencies found when comparing code outputs to corresponding results in the manuscript indicates irreproducibility.

Inconsistencies can appear in several ways, from differences in decimal rounding and mislabeled graphs, to more significant discrepancies as shown in the examples shown below:

[IMAGE]

[IMAGE]

[IMAGE]

> ## Exercise: Comparing Code Outputs to Manuscript Results
>
> Using the manuscript and log file from the previous exercises, compare the code outputs highlighted in the log file and the results highlighted in the manuscript.  List any discrepancies you find between code outputs and corresponding results in the manuscript. 
>
> > ## Solution
> >
> > solution
> >
> {: .solution}
{: .challenge}

The checklist below outlines the tasks completed during the code output and manuscript review as part of the code review component of the curation for reproducibility workflow.

> ## Checklist: Code Output and Manuscript Review
>
> - Review the manuscript, including appendices and supplemental materials, to locate and highlight analysis results displayed in figures, tables, graphs, and in-text numbers.
> - Review the log file to locate and highlight code outputs.
> - Match the highlighted code outputs in the log file to the highlighted results in the manuscript.
> - Compare code outputs to manuscript results to confirm an exact match of numerical results
> - If output files are included in the compendium, compare figures, graphs, and tables in the output files to the figures, graphs, and tables in the manuscript.
> - Document any discrepancy--no matter how minor--found between the code outputs and manuscript results
{: .checklist}

###  Addressing Inconsistencies
When inconsistencies are discovered during the code output and manuscript results comparison, they should be documented in enough detail so that the researcher is able to locate the discrepancy and take corrective actions to resolve the inconsistencies.  
It is highly recommended that researchers run their code and verify their results themselves prior to submitting their compendium for curator review. Researchers know their research best and can spot and address problems more quickly than any third party can.

More often than not, there are simple explanations for these inconsistencies. The vast majority of researchers share their compendia in good faith and with the expectation that their code will reproduce their reported results.  As the first re-users of the research compendium, curators can flag these inconsistencies so that researchers have the opportunity to make corrections *before* sharing it publicly.


{% include links.md %}