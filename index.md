---
layout: lesson
root: .  # Is the only page that doesn't follow the pattern /:path/index.html
permalink: index.html  # Is the only page that doesn't follow the pattern /:path/index.html
---
Preparing for reproducibility occurs throughout the research process. The data management work done during a project's life cycle will lend itself to success when it is time for a data author, curator or journal to assess the reproducibility of a work. 

[Lesson 2: Curating for Reproducibility Workflows]() of the Curating for Reproducibility Curriculum introduced the Data Quality Review (DQR) Framework, a process whereby data and associated files are assessed and required actions are taken to ensure files are independently understandable and reusable. Three of the DQR categories – File, Documentation, and Data Review – work in tandem with Code Review, which is central to assessing the reproducibility of reported research results. 

Assessing reproducibility includes code inspection, code execution, and reviewing the code output against the results presented in the manuscript. As the name suggests, code inspection refers to examining specific aspects of the code. Code execution refers to ensuring that the code runs error-free, which is necessary for verifying that results are reproduced. Manuscript and output review refer to comparing the output produced by the code with the findings reported in the manuscript, that is verifying the reproducibility of the research compendium. 

<!-- this is an html comment -->

{% comment %} This is a comment in Liquid {% endcomment %}

> ## Prerequisites
>
> While this lesson has no prerequisites, completion of the previous lessons of the Curating for Reproducibility Curriculum ([Lesson 1: Introduction to Curating for Reproduciblity]() and [Lesson 2: Curating for Reproducibility Workflows]()) prior to starting this lesson.
{: .prereq}

{% include links.md %}
